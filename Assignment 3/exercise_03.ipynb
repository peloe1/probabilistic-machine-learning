{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8ad31f2f3b619608f4fd3c2d9167ade0",
     "grade": false,
     "grade_id": "cell-81c5a400584e4a8f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## CS-E4825 - Probabilistic Machine Learning D (spring 2025)\n",
    "\n",
    "Pekka Marttinen, Negar Safinianaini, Mihriban Kocak, Bo Zheng, Batuhan Avci.\n",
    "\n",
    "## Exercise 3, due on Friday 31st January at 10:15.\n",
    "\n",
    "### Contents\n",
    "1. Problem 1: Poisson-Gamma\n",
    "2. Problem 2: Multivariate Gaussian\n",
    "3. Problem 3: Posterior of regression weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "38bb2e5ebde49e1760a076b099d6e5a6",
     "grade": false,
     "grade_id": "cell-573bbaa2ef327be0",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## Problem 1: Poisson-Gamma\n",
    "\n",
    "Suppose you have $N$ i.i.d. observations $\\mathbf{x}= \\{x_i\\}_{i=1}^N$ from a $\\operatorname{Poisson}(\\lambda)$ distribution with a rate parameter $\\lambda$ that has a conjugate prior \n",
    "\n",
    "$$\\lambda \\sim \\operatorname{Gamma}(a,b)$$\n",
    "\n",
    "with the shape and rate hyperparameters $a$ and $b$. Derive the posterior distribution $\\lambda|\\bf{x}$.\n",
    "\n",
    "Write your solutions in LateX or attach a picture in the answer cell provided below. You can add a picture using the command ```!(imagename_in_the_folder.jpg)```. Latex in here works similarly as you would write it normally! You can use some of the definitions from the exercise description as a reference. The list of valid Latex commands in Jypyter notebook can be found here: http://www.onemathematicalcat.org/MathJaxDocumentation/TeXSyntax.htm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start out by defining the necessary probability distributions:\n",
    "\n",
    "Prior: $P(\\lambda) = \\frac{b^a}{\\Gamma(a)} \\lambda^{a-1} e^{-b \\lambda}$ ($\\lambda \\sim \\text{Gamma}(a,b)$)\n",
    "\n",
    "Marginal probability: $P(x) = \\int_0^\\infty P(x \\ | \\ \\lambda = u) \\ du = \\int_0^\\infty \\Pi_{i=1}^N \\frac{u^{x_i} e^{-u}}{(x_i)!} du $ due to the fact that the observations in $x$ are i.i.d.\n",
    "\n",
    "Likelihood of $x$ given $\\lambda$: $P(x \\ | \\ \\lambda) = \\Pi_{i=1}^N \\frac{\\lambda^{x_i} e^{-\\lambda}}{(x_i)!}$\n",
    "\n",
    "Let us look at the numerator more closely (likelihood times prior):\n",
    "\n",
    "$P(\\lambda \\ | \\ x) \\propto P(x \\ | \\ \\lambda) P(\\lambda) = \\Pi_{i=1}^N ( \\frac{\\lambda^{x_i} e^{-\\lambda}}{(x_i)!}) \\cdot \\frac{b^a}{\\Gamma(a)} \\lambda^{a-1} e^{-b \\lambda} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(\\lambda \\ | \\ x) \\propto ( \\frac{\\lambda^{\\sum_{i=1}^N x_i} e^{-\\lambda N}}{\\Pi_{i=1}^N (x_i)!}) \\cdot \\frac{b^a}{\\Gamma(a)} \\lambda^{a-1} e^{-b \\lambda} $\n",
    "\n",
    "$P(\\lambda \\ | \\ x) \\propto \\lambda^{a - 1 + \\sum_{i=1}^N x_i} \\ e^{-(b + N) \\lambda} $ with the constants removed\n",
    "\n",
    "$\\Rightarrow \\lambda \\ | \\ x \\sim \\text{Gamma}(a + \\sum_{i=1}^N x_i, b + N) $ \n",
    "\n",
    "$\\lambda \\ | \\ x$ is Gamma distributed with parameters $a + \\sum_{i=1}^N x_i$ and $b+N$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c2d1bd8470ba33c5aa2596654e3cefbc",
     "grade": false,
     "grade_id": "cell-7fdfccb96ae5c3d1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## Problem 2: Multivariate Gaussian\n",
    "\n",
    "Suppose we have $N$ i.i.d. observations $\\mathbf{X} = \\{\\mathbf{x}_i\\}_{i=1}^N$ from a multivariate Gaussian distribution $$\\mathbf{x}_i \\mid \\boldsymbol{\\mu} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$$ with unknown mean parameter $\\boldsymbol{\\mu}$  and a known covariance matrix $\\boldsymbol{\\Sigma}$. As prior information on the mean parameter we have $$ \\boldsymbol{\\mu} \\sim \\mathcal{N}(\\mathbf{m_0}, \\mathbf{S_0}). $$\n",
    "\n",
    "__(a)__ Derive the posterior distribution $p(\\boldsymbol{\\mu}|\\mathbf{X})$ of the mean parameter $\\boldsymbol{\\mu}$. Write your solution in LateX or attach a picture of the solution in the cell below.\n",
    "\n",
    "__(b)__ Compare the Bayesian estimate (posterior mean) to the maximum likelihood estimate by generating $N=10$ observations from the bivariate Gaussian \n",
    "        $$\\mathcal{N}\\left(\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}, \\begin{bmatrix}1 & 0 \\\\ 0 & 1\\end{bmatrix}\\right).$$\n",
    "For this you can use the Python function [numpy.random.normal](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.normal.html), making use of the fact that the elements of the bivariate random vectors are independent. In the Bayesian case, use the prior with $\\mathbf{m_0} = [0,0]^T$ and $\\mathbf{S_0} = [\\begin{smallmatrix}0.1 & 0 \\\\ 0 & 0.1\\end{smallmatrix}]$. Report both estimates. Is the Bayesian estimate closer to the true value $\\boldsymbol{\\mu} = [0,0]^T$? Use the code template given below (after the answer cell) to complete your answer.\n",
    "\n",
    "Write your solutions to __(a)__ and __(b)__ in LateX or attach a picture in the answer cell provided below. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part a)\n",
    "\n",
    "Assuming that the covariance matrix $S_0 \\in \\mathbb{R}^{k \\times k}$ is a positive definite matrix (it is invertible among other nice things), and when $\\mu \\in \\mathbb{R}^k$ is the mean vector. We also know that the resulting posterior distribution should be gaussian given that the likelihood and prior are both gaussian.\n",
    "\n",
    "Prior: $p(\\mu) = \\frac{1}{\\sqrt{(2 \\pi)^k \\text{det}(S_0)}} e^{-\\frac{1}{2} (\\mu - m_0)^\\top S_0^{-1}(\\mu - m_0)}$\n",
    "\n",
    "Likelihood of $X$ given $\\mu$: $p(X \\ | \\ \\mu) = \\Pi_{i=1}^N \\frac{1}{\\sqrt{(2 \\pi)^k \\text{det}(\\Sigma)}} e^{-\\frac{1}{2} (x_i - \\mu)^\\top \\Sigma^{-1}(x_i - \\mu)} = (\\frac{1}{\\sqrt{(2 \\pi)^k \\text{det}(\\Sigma)}})^N \\exp(\\sum_{i=1}^N(-\\frac{1}{2} (x_i - \\mu)^\\top \\Sigma^{-1}(x_i - \\mu)))$\n",
    "\n",
    "$p(X \\ | \\ \\mu) = (\\frac{1}{\\sqrt{(2 \\pi)^k \\text{det}(\\Sigma)}})^N \\exp(-\\frac{1}{2} \\sum_{i=1}^N x_i^\\top \\Sigma^{-1}x_i  - 2 N \\mu^\\top \\Sigma^{-1} \\bar{x} + N \\mu^\\top \\Sigma^{-1} \\mu)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now look at the likelihood times the prior:\n",
    "\n",
    "$p(X \\ | \\ \\mu) \\ p(\\mu) \\propto \\exp(- \\frac{1}{2} (- 2 N \\mu^\\top \\Sigma^{-1} \\bar{x} + N \\mu^\\top \\Sigma^{-1} \\mu) \\  \\text{exp}(-\\frac{1}{2} (\\mu - m_0)^\\top S_0^{-1}(\\mu - m_0))$\n",
    "\n",
    "$p(X \\ | \\ \\mu) \\ p(\\mu) \\propto \\text{exp}(-\\frac{1}{2} (- 2 N \\mu^\\top \\Sigma^{-1} \\bar{x} + N \\mu^\\top \\Sigma^{-1} \\mu + \\mu^\\top S_0^{-1} \\mu - 2 m_0^\\top S_0^{-1} \\mu + m_0^\\top S_0^{-1} m_0) )$\n",
    "\n",
    "$p(X \\ | \\ \\mu) \\ p(\\mu) \\propto \\text{exp}(-\\frac{1}{2} (\\mu^\\top (S_0^{-1} + N \\Sigma^{-1}) \\mu - 2 \\mu^\\top (- N \\Sigma^{-1} \\bar{x} - S_0^{-1} m_0))$\n",
    "\n",
    "$p(X \\ | \\ \\mu) \\ p(\\mu) \\propto  \\text{exp}(-\\frac{1}{2} (\\mu - (S_0^{-1} + N \\Sigma^{-1})^{-1}( S_0^{-1} m_0 + N \\Sigma^{-1} \\bar{x}) ))^\\top (S_0 + N \\Sigma^{-1}) (\\mu - (S_0^{-1} + N \\Sigma^{-1})^{-1}( S_0^{-1} m_0 + N \\Sigma^{-1} \\bar{x}) )   )$\n",
    "\n",
    "Now we notice that the posterior is indeed proportional to a multivariate normal distribution with parameters:\n",
    "\n",
    "\n",
    "$\\Sigma_{\\text{new}} = (S_0^{-1} + N \\Sigma^{-1})^{-1} $ and $\\mu_{\\text{new}} = \\Sigma_{\\text{new}} ( S_0^{-1} m_0 + N \\Sigma^{-1} \\bar{x}) $\n",
    "\n",
    "Thus: $p(X \\ | \\ \\mu) \\ p(\\mu) \\propto  \\text{exp}(-\\frac{1}{2} (\\mu - \\mu_{new})^\\top \\Sigma_{new}^{-1}) (\\mu - \\mu_{new} )) \\Rightarrow X \\ | \\ \\mu \\sim \\mathcal{N}(\\mu_{new}, \\Sigma_{new})$\n",
    "\n",
    "Note in the derivation above I did not bother to list out all of the constants, since they are all absorbed in the normalizing constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af88913931d4649db8324917756a5b72",
     "grade": false,
     "grade_id": "cell-e6a09ef8bf1f72d3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posterior mean:  [-1.43304735  1.58104615]\n",
      "MLE:  [-0.28360705  0.31824903]\n"
     ]
    }
   ],
   "source": [
    "# template for 2(b)\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "\n",
    "m_0 = np.array([0,0])\n",
    "S0 = np.array([[0.1, 0],[0, 0.1]])\n",
    "Sigma = np.array([[1, 0],[0, 1]])\n",
    "N = 10\n",
    "\n",
    "# Sample N bivariate normal vectors\n",
    "# compute MLE and also the posterior mean solution\n",
    "\n",
    "# x = ? #EXERCISE\n",
    "# mle = ? #EXERCISE\n",
    "# posterior_mean = ? #EXERCISE \n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# MLE\n",
    "mu = np.array([0,0])\n",
    "\n",
    "x_sample = np.column_stack((np.random.normal(mu[0], Sigma[0,0], N), np.random.normal(mu[1], Sigma[1,1], N)))\n",
    "\n",
    "mle = np.mean(x_sample, axis=0)\n",
    "\n",
    "\n",
    "# Bayesian\n",
    "Sigma_inv = inv(Sigma)\n",
    "S0_inv = inv(S0)\n",
    "\n",
    "Sigma_new = inv(S0_inv + N * Sigma_inv)\n",
    "mu_new = Sigma_new @ (S0_inv @ m_0 + N * S0_inv @ mle)\n",
    "\n",
    "posterior_sample = np.column_stack((np.random.normal(mu_new[0], Sigma_new[0,0], N), np.random.normal(mu_new[1], Sigma_new[1,1], N)))\n",
    "\n",
    "posterior_mean = np.mean(posterior_sample, axis=0)\n",
    "\n",
    "print(\"Posterior mean: \", posterior_mean)\n",
    "print(\"MLE: \", mle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code the required samplings and computations are done and based on the results the MLE gets closer to the true value of $\\mu = \\begin{bmatrix} 0 & 0 \\end{bmatrix}^\\top$. The posterior mean was: $\\begin{bmatrix} -1.4330 & 1.5810 \\end{bmatrix}^\\top$ and the MLE was $\\begin{bmatrix} -0.2836 & 0.3182 \\end{bmatrix}^\\top$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19765\n",
      "0.29385\n"
     ]
    }
   ],
   "source": [
    "print((10 * 0.3953)/20)\n",
    "print(10 * 0.5877/20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ddf1e85bf2fabec6a07c3676a5945499",
     "grade": false,
     "grade_id": "cell-6f265c79745ea700",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Problem 3: Posterior of regression weights\n",
    "\n",
    "Suppose $y_{i}=\\mathbf{w}^{T}\\mathbf{x}_{i}+\\epsilon_{i},$ for $i=1,\\ldots,n,$ where $\\epsilon_{i}\\sim \\mathcal{N}(0,\\beta^{-1})$. Assume a prior $$\\mathbf{w} \\sim \\mathcal{N} (\\mathbf{0},\\alpha^{-1}\\mathbf{I}).$$ Use 'completing the square' to show that the posterior of $\\mathbf{w}$ is given by $p(\\mathbf{w} \\mid \\mathbf{y}, \\mathbf{x}, \\alpha, \\beta)=\\mathcal{N}(\\mathbf{w} \\mid \\mathbf{m}, \\mathbf{S}),$ where \n",
    "\\begin{align*}\n",
    "    \\mathbf{S} &= \\left( \\alpha \\mathbf{I} + \\beta \\sum_{i=1}^n \\mathbf{x}_i \\mathbf{x}_i^T \\right)^{-1}\\;, \\\\\n",
    "    \\mathbf{m} &= \\beta \\mathbf{S} \\sum_{i=1}^{n} y_i \\mathbf{x}_i.\n",
    "\\end{align*}\n",
    "\n",
    "Write your solution in LateX or attach a picture of the solution in the cell below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prior: \n",
    "\n",
    "$p(w) \\propto \\text{exp}(-\\frac{1}{2} w^\\top (\\alpha^{-1}I)^{-1} w) = \\text{exp}(-\\frac{1}{2} w^\\top (\\alpha I) w)$\n",
    "\n",
    "We know that $y_i$ given $w, x, \\alpha, \\beta$ follows a normal distribution s.t. \n",
    "\n",
    "$y_i \\ | \\ w, x, \\alpha, \\beta \\sim \\mathcal{N}(w^\\top x_i, \\beta^{-1})$\n",
    "\n",
    "Therefore the likelihood is proportional to: \n",
    "\n",
    "$p(y_i \\ | \\ w, x, \\alpha, \\beta) \\propto \\text{exp}(-\\frac{1}{2\\beta} (y_i - w^\\top x_i)^2) $\n",
    "\n",
    "And using that and the assumption of independence between $y_i$'s: \n",
    "\n",
    "$p(y \\ | \\ w, x, \\alpha, \\beta) \\propto \\Pi_{i=1}^N \\text{exp}(-\\frac{\\beta}{2} (y_i - w^\\top x_i)^2) $\n",
    "\n",
    "$p(y \\ | \\ w, x, \\alpha, \\beta) \\propto \\text{exp}(-\\frac{\\beta}{2} \\sum_{i=1}^N (y_i - w^\\top x_i)^2) $\n",
    "\n",
    "Now the likelihood times the prior: \n",
    "\n",
    "$p(y \\ | \\ w, x, \\alpha, \\beta) \\ p(w) \\propto \\text{exp}(-\\frac{\\beta}{2} \\sum_{i=1}^N (y_i - w^\\top x_i)^2) \\ \\text{exp}(-\\frac{1}{2} w^\\top (\\alpha I) w)$\n",
    "\n",
    "$p(y \\ | \\ w, x, \\alpha, \\beta) \\ p(w) \\propto \\text{exp}(-\\frac{1}{2} w^\\top (\\alpha I) w - \\frac{\\beta}{2} \\sum_{i=1}^N (y_i - w^\\top x_i)^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at just the exponent:\n",
    "\n",
    "$-\\frac{1}{2} w^\\top (\\alpha I) w - \\frac{\\beta}{2} \\sum_{i=1}^N (y_i - w^\\top x_i)^2 $\n",
    "\n",
    "$= -\\frac{1}{2} w^\\top (\\alpha I) w - \\frac{\\beta}{2} \\sum_{i=1}^N y_i^2  - 2 w^\\top x_i y_i + (w^\\top x_i)(w^\\top x_i) $\n",
    "\n",
    "$= -\\frac{1}{2} w^\\top (\\alpha I) w - \\frac{\\beta}{2} \\sum_{i=1}^N y_i^2  - 2 w^\\top x_i y_i + (w^\\top (x_i x_i^\\top) w)$\n",
    "\n",
    "$= -\\frac{1}{2} ( w^\\top (\\alpha I) w + \\beta (\\sum_{i=1}^N y_i^2  - 2 w^\\top \\sum_{i=1}^N x_i y_i + (w^\\top (\\sum_{i=1}^N x_i x_i^\\top) w) )  )$\n",
    "\n",
    "$= -\\frac{1}{2} ( w^\\top (\\alpha I + \\beta \\sum_{i=1}^N x_i x_i^\\top) w + \\beta (\\sum_{i=1}^N y_i^2  - 2 w^\\top \\sum_{i=1}^N y_i x_i )  )$\n",
    "\n",
    "$= -(\\frac{1}{2} w^\\top (\\alpha I + \\beta \\sum_{i=1}^N x_i x_i^\\top) w - \\beta (\\sum_{i=1}^N y_i x_i )^\\top w)) - \\frac{\\beta}{2} (\\sum_{i=1}^N y_i^2 )$\n",
    "\n",
    "Now the first two terms are in the form of: $\\frac{1}{2} x^\\top A x - b^\\top x$ for which we can use completing the square to get:  $\\frac{1}{2} (x - A^{-1} b)^\\top A (x - A^{-1} b) - \\frac{1}{2} b^\\top A^{-1} b $ for symmetric matrices $A = A^\\top$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this we get that the exponent is equal to (denote $S = (\\alpha I + \\beta \\sum_{i=1}^N x_i x_i^\\top)^{-1}$ and $m = \\beta S \\sum_{i=1}^N y_i x_i$)\n",
    "\n",
    "$= -\\frac{1}{2} (w-m)^\\top S^{-1} (w-m) + \\frac{\\beta}{2} m^\\top S m - \\frac{\\beta}{2} (\\sum_{i=1}^N y_i^2 )$\n",
    "\n",
    "Now notice that the last two terms are constant w.r.t. to $w$. Thus we can say that:\n",
    "\n",
    "$p(y \\ | \\ w, x, \\alpha, \\beta) \\ p(w) \\propto \\text{exp}(-\\frac{1}{2} (w-m)^\\top S^{-1} (w-m))$\n",
    "\n",
    "Therefore since the posterior is proportional to likelihood times prior, indeed $w \\ | \\ y, x, \\alpha, \\beta \\sim \\mathcal{N}(m, S)$, where $S = (\\alpha I + \\beta \\sum_{i=1}^N x_i x_i^\\top)^{-1}$ and $m = \\beta S \\sum_{i=1}^N y_i x_i$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
