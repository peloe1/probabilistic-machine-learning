# Probabilistic Machine Learning

This repository contains my solutions to the exercises from a graduate-level course on Probabilistic Machine Learning. The course covers constructing Bayesian networks with conditional independence assumptions for modeling joint probability distributions, along with common probabilistic models including sparse Bayesian linear models, Gaussian mixture models, and factor analysis. Key topics include exact and approximate inference algorithms (with emphasis on variational inference), the Expectation-Maximization (EM) algorithm, and latent linear models. Through these exercises, I've gained practical experience applying Bayes theorem for probabilistic inference and implementing complex probabilistic models where exact inference is intractable.


